Verify script env
* Make sure the env/.scripts reflect sample.scripts

Keep on terminal in the root locally (where you execute commands) and one terminal in demo server dina-collections repo


Check server status
* run yarn admin:print:server-status
** If not on nrm connection to internal machines will fail and it will take some time for the command to finish
** use yarn admin:print:server-status -s demo to only print demo server status or -s local to check local server

Load data from sample data locally
* stand in root and make sure backend is not running
* run yarn import:data:sample
* start backend and verify you have data

Create a new json data version
* Edit data in ex .agents (in a way so you can find it in interfaces later). Make sure to edit migrationData and not sourceData
* run yarn admin:build:data:json:zip -s local and provide a new version in the questions
* make sure data.zip is created

Upload the new version to demo
* run yarn admin:upload:data:json -s demo
* run yarn admin:print:server-status -s demo
* Make sure dataFilesVersion reflects the new version name
* ssh into demo and make sure that all files are unpacked from zip-file 

Import the new version in demo
* run yarn admin:import:data:json -s demo -t v0.13.0 (or later tag if exist)
* Go to demo site and verify your changes
* yarn admin:print:server-status -s demo. importVersion and importDate will be added after new images with this branch is deployed

Create sql dump
* Go to demo site and make some changes in the interface (like edit agent name)
* run yarn admin:export:data:sql -s demo
* Make sure dump.sql is created in ./data on the demo server

Download sql dump
* run yarn admin:download:data:sql -s demo
* make sure ./data/dump.sql exist locally and that it is ignored by git

Import sql dump
* Close backend
* run yarn admin:import:data:sql -s local
* start backend
* go into interface and make sure changes you made on demo is there
* run yarn admin:import:data:json -s local and make sure importVersion and importDate makes sense

Deploy old version
* run yarn admin:deploy -s demo -t v0.12.0
* Not that stuff might brack if version is compatible with current data version
* run yarn admin:print:server-status -s demo and make sure correct version is running
* login and check the version flag on logged in home page




* Setup new machine
** docker
*** with sample data

** non docker
*** with sample data

* Deploy new version without data updates
** docker

** non-docker

* Deploy new version with data updates
** docker
*** with sample data
*** with real data

** non-docker
*** with sample data
*** with real data

* Deploy data updates
** docker
*** with sample data
*** with real data

** non-docker
*** with sample data
*** with real data



Scripts
* rm-data-files
* create-data-files-from-sample
* import-data-from-files
* import-data-from-sample-files

* deploy-docker.sh
* deploy-docker-migrate.sh
* deploy-docker-import-from-sample.sh
* deploy-docker-import-from-files.sh

Checks (variable in env files)
* allowDropDatabaseOnMigrations (use to check if drop database is to be run)


Todo
* Update scripts and script envs to reflect updates
* create script that prints info for all servers.
* Remove

Servers
* demo (current alpha)
** migration strategy: docker:import-from-sample

* test (now used as stage)
** migration strategy: docker:import-from-sample

* stage (new)
** migration strategy: docker:migrate:latest

* production
** migration strategy: docker:migrate:latest

* local
** when running docker locally


Package.json
yarn docker:deploy -t=v0.13.0 -s stage
yarn docker:import:data-from-files -t v0.13.0 -s stage
yarn docker:import:data-from-sample -t v0.13.0 -s stage

yarn docker:migrate:latest -t 0.13.0 -s stage
yarn docker:migrate:one -t 0.13.0 -s stage
yarn docker:migrate:undo:one -t 0.13.0 -s stage

yarn import:data-from-files
yarn import:data-from-sample
yarn migrate:latest
yarn migrate:one
yarn migrate:undo:one

yarn import:from-dump
yarn db:dump -s production



yarn docker:deploy -t=my-test -s test


changelog
* 



Order with new data
* 1. create-pr-with-sample-data + add data to correct servers - transport in zip to all servers
* 2. dev tests with data locally
* 3. dev unzipps in the correct servers
* 4. dev do deploystuff
* 5. dev call yarn docker:import-from-sample, docker:import-from-files, yarn docker:migrate depending on env







Search

Implement dropdown component for searching for specific tag
* Talk to anton before
* Should be used at least for storage and taxonomy
1. Implement specification -> Very similar to checkboxSpecification but only work on 1 tag
2. Implement search component -> Need to do an initial fetch like the search checkbox component and then build options. Could also make sense to look at MultipleSearchTagsSelect/TagTypeFilterInlineDropdown. Likely similar props will be needed like the free text and default value option.
3. Add where needed like at taxonomy and storage

Implement new feature range component
* Talk to anton before
* For weight and length
* As it is now range and length dont have any specific components but just the specification
* A new component is needed to enable only matching for the selected type
* Should be quite straight forward to reuse TagTypeFilterInlineDropdown in a new FeatureRangeComponent
* the specification factory needs to be adjusted to forward the type in the queries. the value should be available in sectionValues
>>>>>>> 8326fc3... [BACKEND] Update todo

Add new checkbox for appearance and selective breeding





How does search work?
1. Create a queryBuilder using coreModules/search/utilities/queryBuilderFactory
* As input  querySpecifications which is located in each filter section is used by calling factories from search module



* Search
* 
** Implement multipleTagRefine


Search todo
* make queryBuilder accept arrays in the array and dont spread in all individual querySpecifications





NEXT YEAR
* Implement delete for relationships
* Implement ensure user in controller + fix event row to not have admin as default
* Separate tree items from list items in resource manager. Dont fetch stuff all the time. Use count to check if need to refetch
* Merge resource manager and mammals

Code improvements ideas:
* Use deep diff on json api client with original item to check what to update.
* Make __ROOT__ references point to both resource and id, type object (to allow for required)
* Ensure consistancies in script names ex start:dev build:trees
* Upgrade react, webpack, cra
* Use styleguidist or storybook
* Refactor api clients -> use same in fe and in be
* Components and hocs on module root layer should only contain stuff that is supposed to be exported
* Create hoc for field that is only used in dev or test mode to ensure that field is registered in spec
* Create factories for different types of test for different inputs.
** wrapped in field
** data as integer, boolean, array and what not
* Find good division for general components. Use commonUI Layout or some other module
* In general make navigating folder structures logical
* Use same config setup in frontend and backend
* Potentially remove the apps scoping and only have app
* Determine what should be displayed in storybook


IMPORTANT
* Add all catalog numbers as part of migrations
* Add them as part of migrating specimens
* Add validations to centralized model
* Ensure not possible to run migrations by mistake


TODO
* remove diff
* remove schemaCompliant
* rewrite api tests
* Dra in latest version of data
** use serviceInteractor
** create layer to sync models


Developing a data-model version
1. update model
2. build schema (will be a candidate)
3. add model version migration
5. write migrationFunctions for data up and down
6. run migrations

Build new docker version
1. On master branch run npm version patch, minor, major


Testing a new deploy
1. fetch production db
2. pull latest containers
3. start api and worker
(ensure no edits)
4. run migrations
5. ensure data


For deploy new version
1. pull latest containers
2. restart api and worker (edits unavilable)
3. run migrations
4. api and worker edits available


In Common

* Possible to dump remote db through docker exec?
** https://stackoverflow.com/questions/24718706/backup-restore-a-dockerized-postgresql-database


